# ============================================================
# MCP Doc Fetcher - Enhanced RAG Configuration
# ============================================================

# ============ Ollama Configuration (Local & Fast!) ============
OLLAMA_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_CHAT_MODEL=gpt-oss:20b-cloud

# ============ RAG Strategies (Enable/Disable Features) ============
# Set to "true" or "false"

# Hybrid Search: Combine vector + keyword search for better accuracy
# Benefit: +27% accuracy | Cost: +50ms latency
USE_HYBRID_SEARCH=true

# Contextual Embeddings: LLM-enriched embeddings with document context
# Benefit: +12% precision | Cost: Slow indexing + LLM API calls
# ⚠️  WARNING: This is EXPENSIVE - only enable if you need maximum accuracy
USE_CONTEXTUAL_EMBEDDINGS=false

# Agentic RAG: Extract and index code examples separately
# Benefit: 95% code finding | Cost: Slower crawling + storage
USE_AGENTIC_RAG=true

# Reranking: Cross-encoder reranking for better result ordering
# Benefit: +15% precision | Cost: +100-200ms per query
USE_RERANKING=true

# Knowledge Graph: Neo4j-based hallucination detection
# Benefit: 85% hallucination detection | Cost: Requires Neo4j setup
USE_KNOWLEDGE_GRAPH=true

# ============ Supabase Configuration (Production Vector Storage) ============
# Required for: USE_AGENTIC_RAG, USE_KNOWLEDGE_GRAPH
# Get these from: https://supabase.com/dashboard

SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# ============ Neo4j Configuration (Knowledge Graph) ============
# Required for: USE_KNOWLEDGE_GRAPH=true
# Local install: https://neo4j.com/download/
# Or use free AuraDB: https://neo4j.com/cloud/aura-free/

NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# ============ SQLite Cache (Keep for Speed) ============
CACHE_DB_PATH=./cache/embeddings.db
CACHE_MAX_AGE_HOURS=24

# ============ Performance Tuning ============
# Crawling
MAX_CONCURRENT_CRAWLS=10        # Parallel crawling workers (10-20 recommended)
BATCH_SIZE=20                   # Database batch insert size

# Chunking (Docling Integration)
USE_DOCLING_CHUNKING=true       # Use Docling HybridChunker for token-precise chunking
MAX_TOKENS_PER_CHUNK=512        # Maximum tokens per chunk (must match embedding model)
CHUNKING_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5  # HuggingFace model for tokenizer
MERGE_PEER_CHUNKS=true          # Merge small adjacent chunks

# Legacy Chunking (fallback if Docling disabled)
CHUNK_SIZE=5000                 # Maximum chunk size in characters (legacy)
CHUNK_OVERLAP=200               # Overlap between chunks for continuity (legacy)

# Search
DEFAULT_MATCH_COUNT=5           # Default number of search results
HYBRID_SEARCH_BOOST=1.2         # Boost factor for hybrid search matches
RERANK_TOP_K=20                 # Number of results to rerank

# Code Extraction (for Agentic RAG)
MIN_CODE_BLOCK_LENGTH=300       # Minimum code block size to extract
CODE_CONTEXT_LINES=3            # Lines of context before/after code
MAX_CODE_SUMMARY_TOKENS=200     # Max tokens for code summaries

# ============ Reranking Model ============
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# ============================================================
# Recommended Configurations
# ============================================================

# For AI Coding Assistant (Best Features):
# USE_HYBRID_SEARCH=true
# USE_CONTEXTUAL_EMBEDDINGS=false
# USE_AGENTIC_RAG=true
# USE_RERANKING=true
# USE_KNOWLEDGE_GRAPH=true

# For Fast Basic RAG (Speed Priority):
# USE_HYBRID_SEARCH=true
# USE_CONTEXTUAL_EMBEDDINGS=false
# USE_AGENTIC_RAG=false
# USE_RERANKING=false
# USE_KNOWLEDGE_GRAPH=false

# For Maximum Accuracy (Cost No Object):
# USE_HYBRID_SEARCH=true
# USE_CONTEXTUAL_EMBEDDINGS=true
# USE_AGENTIC_RAG=true
# USE_RERANKING=true
# USE_KNOWLEDGE_GRAPH=true
